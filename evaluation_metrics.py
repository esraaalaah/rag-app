"""
Evaluation & Visualization Script for QA Predictions
----------------------------------------------------

This script loads saved predictions from a CSV file,
computes evaluation metrics (ROUGE, BLEU), and visualizes results.

Metrics included:
    - ROUGE-1: Overlap of unigrams (words) between prediction and reference
    - ROUGE-2: Overlap of bigrams (2-word sequences)
    - ROUGE-L: Longest common subsequence (captures fluency & recall)
    - BLEU: Measures precision of n-grams (common in MT evaluation)

Higher scores are better, but note:
    - ROUGE focuses more on recall (how much reference content was covered).
    - BLEU focuses more on precision (how much predicted text is correct).
"""

import pandas as pd
import evaluate
import matplotlib.pyplot as plt

# ============================
# 1. Load Predictions
# ============================
# Assumes "predictions.csv" contains at least:
#   - reference (gold answer)
#   - bart_prediction (generated by model)
preds_df = pd.read_csv("predictions.csv")

# Extract gold answers and predictions
references = preds_df["reference"].tolist()
systems = {
    "BART": preds_df["bart_prediction"].tolist()
}

# ============================
# 2. Load Metrics
# ============================
rouge = evaluate.load("rouge")
bleu = evaluate.load("bleu")

# ============================
# 3. Compute Scores
# ============================
results = {}
for system_name, preds in systems.items():
    print(f"\nðŸ”¹ Evaluating {system_name}...")

    # ROUGE scores
    rouge_res = rouge.compute(predictions=preds, references=references)

    # BLEU score
    bleu_res = bleu.compute(predictions=preds, references=references)

    # Store results
    results[system_name] = {
        "ROUGE-1": rouge_res["rouge1"],
        "ROUGE-2": rouge_res["rouge2"],
        "ROUGE-L": rouge_res["rougeL"],
        "BLEU": bleu_res["bleu"],
    }

# Convert results to DataFrame for easy visualization
results_df = pd.DataFrame(results).T  # systems Ã— metrics
print("\n===== ðŸ“Š Final Evaluation Results =====")
print(results_df)

# ============================
# 4. Visualization
# ============================

# Plot as grouped bar chart
results_df.plot(kind="bar", figsize=(10, 6))
plt.title("Model Evaluation Metrics")
plt.ylabel("Score")
plt.xlabel("System")
plt.xticks(rotation=0)
plt.legend(title="Metrics")
plt.tight_layout()
plt.savefig("evaluation_scores.png")
plt.show()

print("\nâœ… Visualization saved as evaluation_scores.png")
